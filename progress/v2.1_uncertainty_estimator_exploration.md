# v2.1 — Uncertainty Estimator Exploration

**Date:** 2026-02-28
**Goal:** Find the right aleatoric and epistemic estimators that are truly orthogonal.

## Estimators Tested

### Aleatoric Candidates
1. **Mahalanobis Distance** — distance from calibration mean in covariance-weighted space
   - Problem: responds to BOTH noise AND OOD shifts (not purely aleatoric)
   - Clean: u_a=0.23, Noisy: u_a=0.49, OOD (no noise): u_a=0.71
   - **Rejected** as aleatoric signal

2. **Multi-Sample Variance (MSV)** — variance across N noisy readings of same ground truth
   - Perfect aleatoric signal: noise → high variance, OOD (no noise) → ZERO variance
   - Clean: u_a=0.00, Noisy: u_a=0.73, OOD (no noise): u_a=0.0000
   - **SELECTED** — orthogonal to epistemic BY CONSTRUCTION

### Epistemic Candidates
1. **Spectral Epistemic** — spectral analysis of feature representations
   - Does NOT differentiate OOD well in 36-dim space (stays 0.27-0.32 for all shifts)
   - Needs 256+ dimensional feature space (works in MOT with 256-512 dim features)
   - Effective rank only 2.7-3.1 out of 36
   - **Rejected** for this observation space

2. **Repulsive Epistemic** — repulsive feature analysis
   - Same problem as spectral: low effective dimensionality kills it
   - **Rejected**

3. **Combined (Spectral + Repulsive)** — weighted combination
   - Optimized weights via orthogonality objective
   - Still doesn't differentiate OOD in 36-dim
   - **Rejected**

4. **Mahalanobis Distance** — reused as epistemic signal
   - Measures "unfamiliarity" of states relative to calibration distribution
   - Clean: u_e=0.22, OOD(0.1m): u_e=0.46, OOD(0.3m): u_e=0.71
   - Monotonically increases with OOD severity
   - **SELECTED** — works perfectly as epistemic when paired with MSV for aleatoric

## Orthogonality Results: MSV (aleatoric) + Mahalanobis (epistemic)

### Statistical Tests (noise-sweep only)
| Metric        | Value   | Threshold | Status |
|---------------|---------|-----------|--------|
| Pearson \|r\|   | 0.0037  | < 0.30    | PASS   |
| Spearman \|ρ\|  | 0.0039  | < 0.20    | PASS   |
| HSIC p-value  | 0.995   | > 0.05    | PASS   |

### Behavioral Isolation (the strongest evidence)
| Sweep       | u_a range | u_e range | Isolation Ratio |
|-------------|-----------|-----------|-----------------|
| Noise sweep | 0.44      | 0.0000    | **436,511x**    |
| OOD sweep   | 0.0000    | 0.55      | **553,691x**    |

**Why this works:** MSV measures variance across samples at the same timestep.
If there's no noise, all samples are identical → variance = 0, regardless of OOD state.
Mahalanobis measures distance from calibration mean → unaffected by sample variance.

## Regularization Details
- Covariance regularization: `reg_lambda = 1e-4`
- Condition number: 8.99e+21 → 2.13e+05 after regularization
- Calibration Mahalanobis: mean=4.23, median=2.83, max=63.4

## Files
- `uncertainty/aleatoric.py` — `MultiSampleVarianceEstimator`, `AleatoricEstimator` (Mahalanobis)
- `uncertainty/epistemic.py` — `SpectralEpistemicEstimator`, `RepulsiveEpistemicEstimator` (not used)
- `uncertainty/orthogonality.py` — `OrthogonalityAnalyzer` (Pearson, Spearman, HSIC, CKA)

## Paper Value
- **Section 3 (Method):** MSV for aleatoric, Mahalanobis for epistemic — clear theoretical justification
- **Table 2 (orthogonality):** Pearson=0.004, Spearman=0.004, HSIC p=0.995
- **Figure 2 (behavioral isolation):** Noise sweep and OOD sweep tables — the strongest visual evidence
- **Key claim:** Orthogonal by CONSTRUCTION, not just empirically
